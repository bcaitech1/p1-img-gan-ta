{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, CenterCrop\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/opt/ml/input/data/train/train.csv\")\n",
    "df_test = pd.read_csv(\"/opt/ml/input/data/eval/info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetExample(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return torch.tensor(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "train_image_paths = []\n",
    "for path in train_df[\"path\"][:int(len(train_df[\"path\"]) * 0.9)]:\n",
    "    middle_path = os.path.join(train_image_dir, path)\n",
    "    for file_name in os.listdir(middle_path):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "        train_image_paths.append(os.path.join(middle_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transform(image, title=\"Default\"):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.suptitle(title, fontsize = 16)\n",
    "    \n",
    "    # Unnormalize\n",
    "    image = image / 2 + 0.5  \n",
    "    npimg = image.numpy()\n",
    "    npimg = np.clip(npimg, 0., 1.)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "     ])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title= \"normal image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title= \"noramlize 조정\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop((400,200)),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title= \"crop adjustment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    transforms.RandomRotation(35),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title= \"rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    ToTensor(),\n",
    "    transforms.RandomErasing(p=0.8, scale=(0.2, 0.2)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title= \"vertical flip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    transforms.ColorJitter(brightness=(0.5, 1.5), \n",
    "                               contrast=(0.5, 1.5), \n",
    "                               saturation=(0.5, 1.5)),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    transforms.RandomAffine(30),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    CenterCrop(300),\n",
    "    transforms.RandomAffine(30),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "#     transforms.RandomCrop(height=512, width=512, p=1.0),\n",
    "    transforms.RandomAffine(30),\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transform_al(image, title=\"Default\"):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.suptitle(title, fontsize = 16)\n",
    "    \n",
    "    # Unnormalize \n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    CenterCrop(350, 350),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    RandomCrop(width=256, height=256),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    RandomResizedCrop(width=256, height=256),\n",
    "])\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transform = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.Sequential([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.5),\n",
    "        ]),\n",
    "        A.Sequential([\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "        ]),\n",
    "    ], p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    RandomCrop(height=300, width=300, p=1.0)\n",
    "])\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "            HorizontalFlip(p=1.0),\n",
    "            VerticalFlip(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "            ShiftScaleRotate(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.3, 0.3), p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "            GaussNoise(var_limit=(1000, 1600), p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "            RandomCrop(height=300, width=300, p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.3, 0.3), p=0.5),\n",
    "            GaussNoise(var_limit=(1000, 1600), p=0.3),\n",
    "        ], p=1.0)\n",
    "\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "\n",
    "image = cv2.imread(train_image_paths[0])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "transformed_image = transform(image=image)[\"image\"]\n",
    "show_transform_al(image)\n",
    "show_transform_al(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_path = f'{data_dir}/images/003101_female_Asian_18/mask4.jpg'\n",
    "img = np.array(Image.open(img_path))\n",
    "\n",
    "img_paths = glob(os.path.join(f'{data_dir}/images', '**/*'))\n",
    "random.shuffle(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 4, 4\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(12, 16))\n",
    "\n",
    "trfm_ls = [Blur,\n",
    "           CLAHE,\n",
    "           ChannelDropout,\n",
    "           ChannelShuffle,\n",
    "           ColorJitter,\n",
    "           Equalize,\n",
    "           FancyPCA,\n",
    "           GaussNoise,\n",
    "           GaussianBlur,\n",
    "           GlassBlur,\n",
    "           MedianBlur,\n",
    "           MotionBlur,\n",
    "           MultiplicativeNoise,\n",
    "           RGBShift,\n",
    "           RandomBrightnessContrast,\n",
    "           RandomFog,\n",
    "           Solarize]\n",
    "for i in range(n_rows*n_cols):\n",
    "    trfm = Compose([Resize(height=512, width=384, p=1.0), trfm_ls[i](p=1.0)])\n",
    "    axes[i%n_rows][i//n_cols].imshow(trfm(image=img)['image'])\n",
    "    axes[i%n_rows][i//n_cols].set_title(f'{trfm_ls[i].__name__}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 3, 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(12, 16))\n",
    "\n",
    "trfm_ls = [CoarseDropout,\n",
    "           ElasticTransform,\n",
    "           GridDistortion,\n",
    "           OpticalDistortion,\n",
    "           GridDropout,\n",
    "           HorizontalFlip,\n",
    "           VerticalFlip,\n",
    "           RandomRotate90,\n",
    "           ShiftScaleRotate]\n",
    "for i in range(n_rows*n_cols):\n",
    "    trfm = Compose([Resize(height=512, width=384, p=1.0), trfm_ls[i](p=1.0)])\n",
    "    axes[i%n_rows][i//n_cols].imshow(trfm(image=img)['image'])\n",
    "    axes[i%n_rows][i//n_cols].set_title(f'{trfm_ls[i].__name__}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_url = 'https://prod-images-static.radiopaedia.org/images/13656005/bd937738ad6223a03f8aedcf4920a7_big_gallery.jpeg'\n",
    "img = np.array(Image.open(requests.get(med_url, stream=True).raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = 3, 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(12, 16))\n",
    "\n",
    "trfm_ls = [ElasticTransform,\n",
    "           GridDistortion,\n",
    "           OpticalDistortion,\n",
    "           HorizontalFlip,\n",
    "           GaussNoise,\n",
    "           GaussianBlur,\n",
    "           GlassBlur,\n",
    "           MedianBlur,\n",
    "           MotionBlur]\n",
    "for i in range(n_rows*n_cols):\n",
    "    trfm = Compose([Resize(height=512, width=384, p=1.0), trfm_ls[i](p=1.0)])\n",
    "    axes[i%n_rows][i//n_cols].imshow(trfm(image=img)['image'], cmap='gray')\n",
    "    axes[i%n_rows][i//n_cols].set_title(f'{trfm_ls[i].__name__}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
