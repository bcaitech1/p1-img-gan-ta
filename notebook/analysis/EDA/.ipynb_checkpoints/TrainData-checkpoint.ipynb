{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/opt/ml/input/data/train/train.csv\")\n",
    "df_test = pd.read_csv(\"/opt/ml/input/data/eval/info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "train_image_paths = []\n",
    "for path in train_df[\"path\"]:\n",
    "    middle_path = os.path.join(train_image_dir, path)\n",
    "    for file_name in os.listdir(middle_path):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "        train_image_paths.append(os.path.join(middle_path, file_name))\n",
    "        \n",
    "test_image_paths = []\n",
    "\n",
    "for file_name in os.listdir(test_dir):\n",
    "        if file_name.startswith(\".\"):\n",
    "            continue\n",
    "        test_image_paths.append(os.path.join(test_dir, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "def show_images(paths, n=5, rows = 1, cols = 5, title = 'Default'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    sampleList = random.sample(paths, n)\n",
    "    \n",
    "    for k, path in enumerate(sampleList):\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        # image = resize(image, (200, 200), anti_aliasing=True)\n",
    "\n",
    "        plt.suptitle(title, fontsize = 16)\n",
    "        plt.subplot(rows, cols, k+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_image_paths, n=10, rows=2, cols= 5, title='Train Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "for k, path in enumerate(train_image_paths[:5]):\n",
    "    image = mpimg.imread(path)\n",
    "    \n",
    "    plt.suptitle(\"Original View\", fontsize = 16)\n",
    "    plt.subplot(2, 5, k+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class DatasetExample(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return torch.tensor(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_transform(image, title=\"Default\"):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.suptitle(title, fontsize = 16)\n",
    "    \n",
    "    # Unnormalize\n",
    "    image = image / 2 + 0.5  \n",
    "    npimg = image.numpy()\n",
    "    npimg = np.clip(npimg, 0., 1.)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "     transforms.Resize((100, 100)),\n",
    "     transforms.CenterCrop((100, 100)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "images = next(iter(pytorch_dataloader))\n",
    "\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title=\"Random Vertical Flip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "     transforms.Resize((300, 300)),\n",
    "#      transforms.ColorJitter(brightness=0.7, contrast= 1.7, saturation=0.7),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ])\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "images = next(iter(pytorch_dataloader))\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title=\"Color Jitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "     transforms.Resize((300, 300)),\n",
    "     transforms.RandomGrayscale(p=0.7),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ])\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "pytorch_dataset = DatasetExample(train_image_paths, transform)\n",
    "pytorch_dataloader = DataLoader(dataset=pytorch_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# show images\n",
    "images = next(iter(pytorch_dataloader))\n",
    "show_transform(torchvision.utils.make_grid(images, nrow=6), title=\"Random Greyscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            CenterCrop(350,350),\n",
    "            CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), always_apply=False, p=1.0),\n",
    "#             ColorJitter (brightness=0.7, contrast=0.5, saturation=0.2, hue=0, always_apply=False, p=0.5),\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             ShiftScaleRotate(p=0.5),\n",
    "#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "#             RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "#             GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            CenterCrop(350,350),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (0.56019358, 0.52410121, 0.501457), (0.23318603, 0.24300033, 0.24567522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configurations\n",
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "\n",
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir\n",
    ")\n",
    "dataset.set_transform(transform['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Augmentation으로 이미지를 Normalize했기 때문에, 역으로 다시 Normalize 해주어야합니다.\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip(mean, std)],\n",
    "    std=[1 / s for s in std]\n",
    ")\n",
    "\n",
    "n_rows, n_cols = 4, 3\n",
    "images, labels = next(iter(loader))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
    "for i in range(n_rows*n_cols):\n",
    "    axes[i%n_rows][i//(n_cols+1)].imshow(images[i].permute(1, 2, 0))\n",
    "    axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
