{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import GradScaler, autocast  # 변경 부분\n",
    "\n",
    "model = models.resnet18().cuda()\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Runs the forward pass with autocasting.\n",
    "        with autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n",
    "        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/amp.html\n",
    "# https://pytorch.org/docs/stable/notes/amp_examples.html\n",
    "# https://discuss.pytorch.org/t/optimizer-step-before-lr-scheduler-step-error-using-gradscaler/92930\n",
    "\n",
    "steps = len(train_dl) * epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dl), epochs=epochs)\n",
    "avg_train_losses = []\n",
    "avg_val_losses = []\n",
    "avg_val_scores = []\n",
    "lr = []\n",
    "best_avg_val_score = -1000\n",
    "scaler = torch.cuda.amp.GradScaler() # mixed precision support\n",
    "\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for i, (x, y, image_tensor) in enumerate(train_dl):\n",
    "        x, y, image_tensor = move_to_dev(x, y, image_tensor)\n",
    "        model.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x, image_tensor)\n",
    "            loss = criterion(y, output)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward Pass and Optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        lr.append(get_lr(optimizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
